{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOPqiY/Hlqwi6sdyofj5Atv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peculab/PROGRAM/blob/main/HW4_%E6%96%87%E5%AD%97%E8%B3%87%E6%96%99%E5%B0%8F%E5%88%86%E6%9E%90.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install gspread gspread_dataframe google-auth google-auth-oauthlib google-auth-httplib2 \\\n",
        "               gradio pandas beautifulsoup4 google-generativeai python-dateutil"
      ],
      "metadata": {
        "id": "8dhA2pQxhLLX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, time, uuid, re, json, datetime\n",
        "from datetime import datetime as dt, timedelta\n",
        "from dateutil.tz import gettz\n",
        "import pandas as pd\n",
        "import gradio as gr\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "import google.generativeai as genai\n",
        "\n",
        "# Google Auth & Sheets\n",
        "from google.colab import auth\n",
        "import gspread\n",
        "from gspread_dataframe import set_with_dataframe, get_as_dataframe\n",
        "from google.auth.transport.requests import Request\n",
        "from google.oauth2 import service_account\n",
        "from google.auth import default"
      ],
      "metadata": {
        "id": "Niq4EB1Ui-Pk"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "import gspread\n",
        "from google.auth import default\n",
        "creds, _ = default()\n",
        "\n",
        "gc = gspread.authorize(creds)"
      ],
      "metadata": {
        "id": "PGiNuVijQ-Yb"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "# å¾ Colab Secrets ä¸­ç²å– API é‡‘é‘°\n",
        "api_key = userdata.get('gemini')\n",
        "\n",
        "# ä½¿ç”¨ç²å–çš„é‡‘é‘°é…ç½® genai\n",
        "genai.configure(api_key=api_key)\n",
        "\n",
        "model = genai.GenerativeModel('gemini-2.5-pro')"
      ],
      "metadata": {
        "id": "XjPjVJFnkUCk"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SHEET_URL = \"https://docs.google.com/spreadsheets/d/1jR3qRQr2ZvWYKNuv8wen_-eTZWdc5a-LLvH7iymn2zw/edit?usp=sharing\"\n",
        "WORKSHEET_NAME = \"å·¥ä½œè¡¨4\"\n",
        "TIMEZONE = \"Asia/Taipei\""
      ],
      "metadata": {
        "id": "_RATk0PYi-q9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PTT_HEADER = [\n",
        "    \"post_id\",\"title\",\"url\",\"date\",\"author\",\"nrec\",\"created_at\",\n",
        "    \"fetched_at\",\"content\"\n",
        "]\n",
        "TERMS_HEADER = [\"term\",\"freq\",\"df_count\",\"tfidf_mean\",\"examples\"]"
      ],
      "metadata": {
        "id": "5ROKlgT_Y4kn"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ensure_spreadsheet(name):\n",
        "    try:\n",
        "        sh = gc.open(name)  # returns gspread.models.Spreadsheet\n",
        "    except gspread.SpreadsheetNotFound:\n",
        "        sh = gc.create(name)\n",
        "    return sh\n",
        "\n",
        "sh = ensure_spreadsheet(WORKSHEET_NAME)"
      ],
      "metadata": {
        "id": "vbhukYyIUOVK"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ensure_worksheet(sh, title, header):\n",
        "    try:\n",
        "        ws = sh.worksheet(title)\n",
        "    except gspread.WorksheetNotFound:\n",
        "        ws = sh.add_worksheet(title=title, rows=\"1000\", cols=str(len(header)+5))\n",
        "        ws.update([header])\n",
        "    # è‹¥æ²’æœ‰è¡¨é ­å°±è£œä¸Š\n",
        "    data = ws.get_all_values()\n",
        "    if not data or (data and data[0] != header):\n",
        "        ws.clear()\n",
        "        ws.update([header])\n",
        "    return ws"
      ],
      "metadata": {
        "id": "4ntDPlpMaR-R"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ws_ptt_posts = ensure_worksheet(sh, \"ptt_movie_posts\", PTT_HEADER)\n",
        "ws_ptt_terms = ensure_worksheet(sh, \"ptt_movie_terms\", TERMS_HEADER)"
      ],
      "metadata": {
        "id": "V4kkOqg9Y7M8"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============\n",
        "# PTT é›»å½±ç‰ˆçˆ¬èŸ²\n",
        "# ==============\n",
        "PTT_MOVIE_INDEX = \"https://www.ptt.cc/bbs/movie/index.html\"\n",
        "PTT_COOKIES = {\"over18\": \"1\"}\n",
        "\n",
        "def _get_soup(url):\n",
        "    r = requests.get(url, timeout=15, headers={\"User-Agent\":\"Mozilla/5.0\"}, cookies=PTT_COOKIES)\n",
        "    r.raise_for_status()\n",
        "    return BeautifulSoup(r.text, \"html.parser\")\n",
        "\n",
        "def _get_prev_index_url(soup):\n",
        "    btns = soup.select(\"div.btn-group-paging a.btn.wide\")\n",
        "    # é é¢é€šå¸¸æœ‰ä¸‰å€‹ï¼šæœ€èˆŠ â† ä¸Šä¸€é  â† æœ€å‰é ï¼›å–ã€Œä¸Šä¸€é ã€\n",
        "    for a in btns:\n",
        "        if \"ä¸Šé \" in a.get_text(strip=True):\n",
        "            href = a.get(\"href\")\n",
        "            if href:\n",
        "                return \"https://www.ptt.cc\" + href\n",
        "    return None\n",
        "\n",
        "def _parse_nrec(nrec_span):\n",
        "    # å¯èƒ½æ˜¯æ•¸å­—æˆ– \"çˆ†\"/\"X1\" ç­‰\n",
        "    if not nrec_span:\n",
        "        return 0\n",
        "    txt = nrec_span.get_text(strip=True)\n",
        "    if txt == \"çˆ†\":\n",
        "        return 100\n",
        "    if txt.startswith(\"X\"):\n",
        "        try:\n",
        "            return -int(txt[1:])\n",
        "        except:\n",
        "            return -10\n",
        "    try:\n",
        "        return int(txt)\n",
        "    except:\n",
        "        return 0\n",
        "\n",
        "def _extract_post_list(soup):\n",
        "    posts = []\n",
        "    for r in soup.select(\"div.r-ent\"):\n",
        "        a = r.select_one(\"div.title a\")\n",
        "        if not a:\n",
        "            continue\n",
        "        title = a.get_text(strip=True)\n",
        "        url = \"https://www.ptt.cc\" + a.get(\"href\")\n",
        "        author = r.select_one(\"div.author\").get_text(strip=True)\n",
        "        date = r.select_one(\"div.date\").get_text(strip=True)\n",
        "        nrec = _parse_nrec(r.select_one(\"div.nrec span\"))\n",
        "        posts.append({\n",
        "            \"title\": title, \"url\": url, \"author\": author, \"date\": date, \"nrec\": nrec\n",
        "        })\n",
        "    return posts\n",
        "\n",
        "def _clean_ptt_content(soup):\n",
        "    # ç§»é™¤æ¨æ–‡å€\n",
        "    for p in soup.select(\"div.push\"):\n",
        "        p.decompose()\n",
        "    main = soup.select_one(\"#main-content\")\n",
        "    if not main:\n",
        "        return \"\", \"\"\n",
        "    # ç§»é™¤çœ‹æ¿çš„ meta è³‡è¨Šè¡Œï¼ˆä½œè€…/æ¨™é¡Œ/æ™‚é–“ï¼‰\n",
        "    metas = main.select(\"div.article-metaline, div.article-metaline-right\")\n",
        "    for m in metas:\n",
        "        m.decompose()\n",
        "    # å–å‡ºå…§æ–‡èˆ‡ç°½åæª”å‰åˆ‡å‰²\n",
        "    text = main.get_text(\"\\n\", strip=True)\n",
        "    if \"--\" in text:\n",
        "        text = text.split(\"--\")[0].strip()\n",
        "    # æ“·å–æ¨™é¡Œï¼ˆæœ‰äº›æ–‡ç« æ¨™é¡Œå¯å†è£œæ•‘ï¼‰\n",
        "    title_tag = soup.select_one(\"span.article-meta-value\")\n",
        "    meta_title = title_tag.get_text(strip=True) if title_tag else \"\"\n",
        "    return text, meta_title\n",
        "\n",
        "def crawl_ptt_movie(index_pages=3, min_push=0, keyword=\"\"):\n",
        "    \"\"\"å¾æœ€æ–° index.html å¾€å‰ç¿» index_pages é ï¼ŒæŠ“æ»¿è¶³æ¢ä»¶çš„æ–‡ç« \"\"\"\n",
        "    global ptt_posts_df\n",
        "    url = PTT_MOVIE_INDEX\n",
        "    all_rows = []\n",
        "    seen_urls = set(ptt_posts_df[\"url\"].tolist()) if not ptt_posts_df.empty else set()\n",
        "\n",
        "    for _ in range(int(index_pages)):\n",
        "        soup = _get_soup(url)\n",
        "        posts = _extract_post_list(soup)\n",
        "        # ç¯©é¸\n",
        "        for p in posts:\n",
        "            if p[\"nrec\"] < int(min_push):\n",
        "                continue\n",
        "            if keyword and (keyword not in p[\"title\"]):\n",
        "                continue\n",
        "            # å»é‡ï¼ˆé¿å…åè¦†æŠ“åŒä¸€ç¯‡ï¼‰\n",
        "            if p[\"url\"] in seen_urls:\n",
        "                continue\n",
        "\n",
        "            # æŠ“æ­£æ–‡\n",
        "            try:\n",
        "                art_soup = _get_soup(p[\"url\"])\n",
        "                content, meta_title = _clean_ptt_content(art_soup)\n",
        "            except Exception as e:\n",
        "                content, meta_title = \"\", \"\"\n",
        "            final_title = p[\"title\"] if p[\"title\"] else (meta_title or \"ï¼ˆç„¡æ¨™é¡Œï¼‰\")\n",
        "\n",
        "            all_rows.append({\n",
        "                \"post_id\": str(uuid.uuid4())[:8],\n",
        "                \"title\": final_title[:200],\n",
        "                \"url\": p[\"url\"],\n",
        "                \"date\": p[\"date\"],\n",
        "                \"author\": p[\"author\"],\n",
        "                \"nrec\": str(p[\"nrec\"]),\n",
        "                \"created_at\": tznow().isoformat(),\n",
        "                \"fetched_at\": tznow().isoformat(),\n",
        "                \"content\": content\n",
        "            })\n",
        "\n",
        "        # æ›ä¸Šä¸€é \n",
        "        prev = _get_prev_index_url(soup)\n",
        "        if not prev:\n",
        "            break\n",
        "        url = prev\n",
        "\n",
        "    if all_rows:\n",
        "        new_df = pd.DataFrame(all_rows, columns=PTT_HEADER)\n",
        "        ptt_posts_df = pd.concat([ptt_posts_df, new_df], ignore_index=True)\n",
        "        write_df(ws_ptt_posts, ptt_posts_df, PTT_HEADER)\n",
        "        return f\"âœ… å–å¾— {len(all_rows)} ç¯‡æ–‡ç« ï¼ˆå·²å¯«å…¥ Sheetï¼‰\", ptt_posts_df\n",
        "    else:\n",
        "        return \"â„¹ï¸ æ²’æœ‰æ–°æ–‡ç« ç¬¦åˆæ¢ä»¶ï¼ˆæˆ–å…§å®¹å·²åœ¨ Sheetï¼‰\", ptt_posts_df\n",
        "\n",
        "\n",
        "# ==============\n",
        "# æ–‡æœ¬åˆ†æï¼ˆjieba + TF/IDF + bigramï¼‰\n",
        "# ==============\n",
        "import re\n",
        "try:\n",
        "    import jieba\n",
        "except:\n",
        "    jieba = None\n",
        "\n",
        "def _tokenize_zh(text):\n",
        "    text = re.sub(r\"[^\\u4e00-\\u9fffA-Za-z0-9]+\", \" \", text)\n",
        "    if not jieba:\n",
        "        # å¾Œå‚™ï¼šç”¨ç©ºç™½åˆ‡ï¼ˆè¼ƒå·®ï¼Œä½†é¿å…ç„¡æ³•åŸ·è¡Œï¼‰\n",
        "        return [t for t in text.split() if len(t) > 1]\n",
        "    return [w.strip() for w in jieba.lcut(text) if len(w.strip()) > 1]\n",
        "\n",
        "def analyze_ptt_texts(topk=50, min_df=2):\n",
        "    global ptt_posts_df, terms_df\n",
        "    if ptt_posts_df.empty:\n",
        "        return \"ğŸ“­ å°šç„¡å·²æŠ“å–çš„æ–‡ç« ï¼Œè«‹å…ˆåœ¨ã€PTT é›»å½±çˆ¬èŸ²ã€åˆ†é å–å¾—æ–‡ç« ã€‚\", terms_df, \"\"\n",
        "\n",
        "    docs = []\n",
        "    for _, r in ptt_posts_df.iterrows():\n",
        "        # å°‡æ¨™é¡Œèˆ‡å…§æ–‡æ‹¼èµ·ä¾†æé«˜é—œéµè©å¯è¦‹åº¦\n",
        "        docs.append((r[\"title\"] or \"\") + \"\\n\" + (r[\"content\"] or \"\"))\n",
        "\n",
        "    # è©é »\n",
        "    from collections import Counter, defaultdict\n",
        "    freq = Counter()\n",
        "    df_cnt = defaultdict(int)\n",
        "\n",
        "    token_docs = []\n",
        "    for doc in docs:\n",
        "        toks = _tokenize_zh(doc)\n",
        "        token_docs.append(toks)\n",
        "        freq.update(toks)\n",
        "        for t in set(toks):\n",
        "            df_cnt[t] += 1\n",
        "\n",
        "    # TF-IDFï¼ˆå¹³å‡å€¼ï¼‰\n",
        "    try:\n",
        "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "        vec = TfidfVectorizer(tokenizer=_tokenize_zh, lowercase=False, min_df=min_df)\n",
        "        X = vec.fit_transform(docs)\n",
        "        terms = vec.get_feature_names_out()\n",
        "        tfidf_mean = X.mean(axis=0).A1\n",
        "        tfidf_map = dict(zip(terms, tfidf_mean))\n",
        "    except Exception:\n",
        "        tfidf_map = {t: 0.0 for t in freq.keys()}\n",
        "\n",
        "    # Bigramï¼ˆç²—ç•¥ï¼‰\n",
        "    from itertools import tee\n",
        "    def pairwise(iterable):\n",
        "        a, b = tee(iterable)\n",
        "        next(b, None)\n",
        "        return zip(a, b)\n",
        "    bigram_freq = Counter()\n",
        "    for toks in token_docs:\n",
        "        bigram_freq.update([\" \".join(bg) for bg in pairwise(toks)])\n",
        "\n",
        "    # å– TopK é—œéµè©ï¼ˆç¶œåˆï¼šå…ˆæŒ‰ tfidfï¼Œå†ç”¨ freq ç•¶æ¬¡æ’åºï¼‰\n",
        "    candidates = list(freq.keys())\n",
        "    candidates.sort(key=lambda t: (round(tfidf_map.get(t,0.0), 6), freq[t]), reverse=True)\n",
        "    top_terms = candidates[:int(topk)]\n",
        "\n",
        "    # ç¯„ä¾‹å¥ï¼ˆç°¡å–®å¾ä»»ä¸€ç¯‡å– 1 å€‹æ¨£ä¾‹ç‰‡æ®µï¼‰\n",
        "    examples = {}\n",
        "    for term in top_terms:\n",
        "        ex = \"\"\n",
        "        for doc in docs:\n",
        "            if term in doc:\n",
        "                # å–å‡ºåŒ…å«è©²è©çš„ç‰‡æ®µï¼ˆÂ±15å­—ï¼‰\n",
        "                i = doc.find(term)\n",
        "                s = max(0, i-15)\n",
        "                e = min(len(doc), i+len(term)+15)\n",
        "                ex = doc[s:e].replace(\"\\n\",\" \")\n",
        "                break\n",
        "        examples[term] = ex\n",
        "\n",
        "    rows = []\n",
        "    for t in top_terms:\n",
        "        rows.append({\n",
        "            \"term\": t,\n",
        "            \"freq\": str(freq[t]),\n",
        "            \"df_count\": str(df_cnt[t]),\n",
        "            \"tfidf_mean\": f\"{tfidf_map.get(t,0.0):.6f}\",\n",
        "            \"examples\": examples.get(t, \"\")\n",
        "        })\n",
        "    terms_df = pd.DataFrame(rows, columns=TERMS_HEADER)\n",
        "    write_df(ws_ptt_terms, terms_df, TERMS_HEADER)\n",
        "\n",
        "    # ç”¢ç”Ÿ Markdown æ‘˜è¦\n",
        "    md_lines = []\n",
        "    md_lines.append(f\"### é—œéµè© Top {len(top_terms)}ï¼ˆä¾ TF-IDF å¹³å‡å€¼å„ªå…ˆï¼Œæ¬¡åºå†ä»¥è©é »ï¼‰\")\n",
        "    for i, t in enumerate(top_terms, 1):\n",
        "        md_lines.append(f\"{i}. **{t}** â€” tfidfâ‰ˆ{float(tfidf_map.get(t,0.0)):.4f}ï¼›freq={freq[t]}ï¼›df={df_cnt[t]}\")\n",
        "    md_lines.append(\"\\n### å¸¸è¦‹é›™è©æ­é…ï¼ˆå‰ 20ï¼‰\")\n",
        "    for i, (bg, c) in enumerate(bigram_freq.most_common(20), 1):\n",
        "        md_lines.append(f\"{i}. {bg} â€” {c}\")\n",
        "\n",
        "    return f\"âœ… å·²å®Œæˆæ–‡æœ¬åˆ†æï¼Œå…± {len(docs)} ç¯‡æ–‡ç« ï¼›é—œéµè©å·²å¯«å…¥ Sheetã€‚\", terms_df, \"\\n\".join(md_lines)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Z_zHAaXZIk_",
        "outputId": "3df19f8a-7aad-4365-cb3c-70ef00f11126"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jieba/__init__.py:44: SyntaxWarning: invalid escape sequence '\\.'\n",
            "  re_han_default = re.compile(\"([\\u4E00-\\u9FD5a-zA-Z0-9+#&\\._%\\-]+)\", re.U)\n",
            "/usr/local/lib/python3.12/dist-packages/jieba/__init__.py:46: SyntaxWarning: invalid escape sequence '\\s'\n",
            "  re_skip_default = re.compile(\"(\\r\\n|\\s)\", re.U)\n",
            "/usr/local/lib/python3.12/dist-packages/jieba/finalseg/__init__.py:78: SyntaxWarning: invalid escape sequence '\\.'\n",
            "  re_skip = re.compile(\"([a-zA-Z0-9]+(?:\\.\\d+)?%?)\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TASKS_HEADER = [\n",
        "    \"id\",\"task\",\"status\",\"priority\",\"est_min\",\"start_time\",\"end_time\",\n",
        "    \"actual_min\",\"pomodoros\",\"due_date\",\"labels\",\"notes\",\n",
        "    \"created_at\",\"updated_at\",\"completed_at\",\"planned_for\"\n",
        "]\n",
        "LOGS_HEADER = [\n",
        "    \"log_id\",\"task_id\",\"phase\",\"start_ts\",\"end_ts\",\"minutes\",\"cycles\",\"note\"\n",
        "]\n",
        "CLIPS_HEADER = [\"clip_id\",\"url\",\"selector\",\"text\",\"href\",\"created_at\",\"added_to_task\"]\n",
        "\n",
        "ws_tasks = ensure_worksheet(sh, \"tasks\", TASKS_HEADER)\n",
        "ws_logs  = ensure_worksheet(sh, \"pomodoro_logs\", LOGS_HEADER)\n",
        "ws_clips = ensure_worksheet(sh, \"web_clips\", CLIPS_HEADER)\n",
        "\n",
        "def tznow():\n",
        "    return dt.now(gettz(TIMEZONE))\n",
        "\n",
        "def read_df(ws, header):\n",
        "    df = get_as_dataframe(ws, evaluate_formulas=True, header=0)\n",
        "    if df is None or df.empty:\n",
        "        return pd.DataFrame(columns=header)\n",
        "    df = df.fillna(\"\")\n",
        "    # ä¿è­‰æ¬„ä½é½Šå…¨\n",
        "    for c in header:\n",
        "        if c not in df.columns:\n",
        "            df[c] = \"\"\n",
        "    # å‹åˆ¥å¾®èª¿\n",
        "    if \"est_min\" in df.columns:\n",
        "        df[\"est_min\"] = pd.to_numeric(df[\"est_min\"], errors=\"coerce\").fillna(0).astype(int)\n",
        "    if \"actual_min\" in df.columns:\n",
        "        df[\"actual_min\"] = pd.to_numeric(df[\"actual_min\"], errors=\"coerce\").fillna(0).astype(int)\n",
        "    if \"pomodoros\" in df.columns:\n",
        "        df[\"pomodoros\"] = pd.to_numeric(df[\"pomodoros\"], errors=\"coerce\").fillna(0).astype(int)\n",
        "    return df[header]\n",
        "\n",
        "def write_df(ws, df, header):\n",
        "    if df.empty:\n",
        "        ws.clear()\n",
        "        ws.update([header])\n",
        "        return\n",
        "    # è½‰å­—ä¸²é¿å… gspread å‹åˆ¥å•é¡Œ\n",
        "    df_out = df.copy()\n",
        "    for c in df_out.columns:\n",
        "        df_out[c] = df_out[c].astype(str)\n",
        "    ws.clear()\n",
        "    ws.update([header] + df_out.values.tolist())\n",
        "\n",
        "def refresh_all():\n",
        "    return (\n",
        "        read_df(ws_tasks, TASKS_HEADER).copy(),\n",
        "        read_df(ws_logs, LOGS_HEADER).copy(),\n",
        "        read_df(ws_clips, CLIPS_HEADER).copy()\n",
        "    )\n",
        "\n",
        "tasks_df, logs_df, clips_df = refresh_all()\n",
        "\n",
        "def add_task(task, priority, est_min, due_date, labels, notes, planned_for):\n",
        "    global tasks_df\n",
        "    _now = tznow().isoformat()\n",
        "    new = pd.DataFrame([{\n",
        "        \"id\": str(uuid.uuid4())[:8],\n",
        "        \"task\": task.strip(),\n",
        "        \"status\": \"todo\",\n",
        "        \"priority\": priority or \"M\",\n",
        "        \"est_min\": int(est_min) if est_min else 25,\n",
        "        \"start_time\": \"\",\n",
        "        \"end_time\": \"\",\n",
        "        \"actual_min\": 0,\n",
        "        \"pomodoros\": 0,\n",
        "        \"due_date\": due_date or \"\",\n",
        "        \"labels\": labels or \"\",\n",
        "        \"notes\": notes or \"\",\n",
        "        \"created_at\": _now,\n",
        "        \"updated_at\": _now,\n",
        "        \"completed_at\": \"\",\n",
        "        \"planned_for\": planned_for or \"\"  # å¯å¡« today / tomorrow / ç©ºç™½\n",
        "    }])\n",
        "    tasks_df = pd.concat([tasks_df, new], ignore_index=True)\n",
        "    write_df(ws_tasks, tasks_df, TASKS_HEADER)\n",
        "    return \"âœ… å·²æ–°å¢ä»»å‹™\", tasks_df\n",
        "\n",
        "def update_task_status(task_id, new_status):\n",
        "    global tasks_df\n",
        "    idx = tasks_df.index[tasks_df[\"id\"] == task_id]\n",
        "    if len(idx)==0:\n",
        "        return \"âš ï¸ æ‰¾ä¸åˆ°ä»»å‹™\", tasks_df\n",
        "    i = idx[0]\n",
        "    tasks_df.loc[i, \"status\"] = new_status\n",
        "    tasks_df.loc[i, \"updated_at\"] = tznow().isoformat()\n",
        "    if new_status == \"done\" and not tasks_df.loc[i, \"completed_at\"]:\n",
        "        tasks_df.loc[i, \"completed_at\"] = tznow().isoformat()\n",
        "    write_df(ws_tasks, tasks_df, TASKS_HEADER)\n",
        "    return \"âœ… ç‹€æ…‹å·²æ›´æ–°\", tasks_df\n",
        "\n",
        "def mark_done(task_id):\n",
        "    return update_task_status(task_id, \"done\")\n",
        "\n",
        "def recalc_task_actuals(task_id):\n",
        "    \"\"\"æ ¹æ“š logs_df å›å¯« actual_min èˆ‡ pomodoros\"\"\"\n",
        "    global tasks_df, logs_df\n",
        "    work_logs = logs_df[(logs_df[\"task_id\"]==task_id) & (logs_df[\"phase\"]==\"work\")]\n",
        "    total_min = work_logs[\"minutes\"].astype(float).sum() if not work_logs.empty else 0\n",
        "    pomos = int(round(total_min / 25.0))\n",
        "    idx = tasks_df.index[tasks_df[\"id\"]==task_id]\n",
        "    if len(idx)==0: return\n",
        "    i = idx[0]\n",
        "    tasks_df.loc[i,\"actual_min\"] = int(total_min)\n",
        "    tasks_df.loc[i,\"pomodoros\"] = pomos\n",
        "    tasks_df.loc[i,\"updated_at\"] = tznow().isoformat()\n",
        "\n",
        "def list_task_choices():\n",
        "    global tasks_df\n",
        "    if tasks_df.empty:\n",
        "        return []\n",
        "    # é¡¯ç¤ºï¼š [status] (P:priority) task  â€” id\n",
        "    def row_label(r):\n",
        "        return f\"[{r['status']}] (P:{r['priority']}) {r['task']} â€” {r['id']}\"\n",
        "    return [(row_label(r), r[\"id\"]) for _, r in tasks_df.iterrows()]\n",
        "\n",
        "# æˆ‘å€‘æ¡ã€ŒæŒ‰éˆ•é–‹å§‹ / çµæŸã€æ¨¡å¼ï¼ˆé¿å…å¾Œç«¯é˜»å¡ï¼‰ï¼Œæ¯æ¬¡æŒ‰ã€Œé–‹å§‹ã€æœƒå…ˆè¨˜ä½ start_tsï¼Œ\n",
        "# æŒ‰ã€ŒçµæŸã€æ™‚è¨ˆç®—åˆ†é˜ä¸¦å¯«å…¥ logsï¼Œå†å›å¡«ä»»å‹™ actual_min / pomodorosã€‚\n",
        "\n",
        "_active_sessions = {}  # { task_id: {\"phase\": \"work\"/\"break\", \"start_ts\": iso, \"cycles\": int} }\n",
        "\n",
        "def start_phase(task_id, phase, cycles):\n",
        "    if not task_id: return \"âš ï¸ è«‹å…ˆé¸æ“‡ä»»å‹™\"\n",
        "    _active_sessions[task_id] = {\n",
        "        \"phase\": phase,\n",
        "        \"start_ts\": tznow().isoformat(),\n",
        "        \"cycles\": int(cycles) if cycles else 1\n",
        "    }\n",
        "    return f\"â–¶ï¸ å·²é–‹å§‹ï¼š{phase}ï¼ˆtask: {task_id}ï¼‰\"\n",
        "\n",
        "def end_phase(task_id, note):\n",
        "    global logs_df, tasks_df\n",
        "    if task_id not in _active_sessions:\n",
        "        return \"âš ï¸ å°šæœªé–‹å§‹ä»»ä½•éšæ®µ\"\n",
        "    sess = _active_sessions.pop(task_id)\n",
        "    start = pd.to_datetime(sess[\"start_ts\"])\n",
        "    end = tznow()\n",
        "    minutes = round((end - start).total_seconds() / 60.0, 2)\n",
        "    log = pd.DataFrame([{\n",
        "        \"log_id\": str(uuid.uuid4())[:8],\n",
        "        \"task_id\": task_id,\n",
        "        \"phase\": sess[\"phase\"],\n",
        "        \"start_ts\": start.isoformat(),\n",
        "        \"end_ts\": end.isoformat(),\n",
        "        \"minutes\": minutes,\n",
        "        \"cycles\": int(sess[\"cycles\"]),\n",
        "        \"note\": note or \"\"\n",
        "    }])\n",
        "    logs_df = pd.concat([logs_df, log], ignore_index=True)\n",
        "    write_df(ws_logs, logs_df, LOGS_HEADER)\n",
        "\n",
        "    # å›å¡«ä»»å‹™\n",
        "    if sess[\"phase\"] == \"work\":\n",
        "        recalc_task_actuals(task_id)\n",
        "        write_df(ws_tasks, tasks_df, TASKS_HEADER)\n",
        "\n",
        "    return f\"â¹ï¸ å·²çµæŸï¼š{sess['phase']}ï¼Œç´€éŒ„ {minutes} åˆ†é˜\"\n",
        "\n",
        "# AI è¨ˆç•«ï¼ˆGeminiï¼›ç„¡é‡‘é‘°å‰‡è¦å‰‡å¼ï¼‰\n",
        "def generate_today_plan():\n",
        "    global tasks_df\n",
        "    # ä»¥ã€Œdue_date æ˜¯ä»Šå¤©ã€æˆ–ã€Œplanned_for = todayã€ä¸”ä¸æ˜¯ done çš„ä»»å‹™ç‚ºè¨ˆç•«æ¸…å–®\n",
        "    today = tznow().date().isoformat()\n",
        "    cand = tasks_df[\n",
        "        ((tasks_df[\"due_date\"]==today) | (tasks_df[\"planned_for\"].str.lower()==\"today\")) &\n",
        "        (tasks_df[\"status\"]!=\"done\")\n",
        "    ].copy()\n",
        "    if cand.empty:\n",
        "        return \"ğŸ“­ ä»Šå¤©æ²’æœ‰æ¨™è¨˜çš„ä»»å‹™ã€‚è«‹åœ¨ Tasks åˆ†é æŠŠä»»å‹™çš„ due_date è¨­ç‚ºä»Šå¤©æˆ– planned_for è¨­ç‚º todayã€‚\"\n",
        "\n",
        "    # å…ˆä¾ priorityï¼ˆH>M>Lï¼‰+ est_min æ’åº\n",
        "    pr_order = {\"H\":0, \"M\":1, \"L\":2}\n",
        "    cand[\"p_ord\"] = cand[\"priority\"].map(pr_order).fillna(3)\n",
        "    cand = cand.sort_values([\"p_ord\",\"est_min\"], ascending=[True, True])\n",
        "\n",
        "    # å˜—è©¦ Gemini\n",
        "    api_key = os.environ.get(\"GEMINI_API_KEY\",\"\").strip()\n",
        "    if api_key:\n",
        "        genai.configure(api_key=api_key)\n",
        "        sys_prompt = (\n",
        "            \"ä½ æ˜¯ä¸€ä½ä»»å‹™è¦åŠƒåŠ©ç†ã€‚è«‹æŠŠè¼¸å…¥çš„ä»»å‹™ï¼ˆå«ä¼°æ™‚èˆ‡å„ªå…ˆç´šï¼‰æ’æˆä¸‰æ®µï¼šmorningã€afternoonã€eveningï¼Œ\"\n",
        "            \"ä¸¦çµ¦å‡ºæ¯æ®µçš„é‡é»ã€é †åºã€æ¯é …çš„æ™‚é–“é ä¼°èˆ‡å‚™è¨»ã€‚ç¸½æ™‚æ•¸è«‹å¤§è‡´ç¬¦åˆä»»å‹™ä¼°æ™‚ç¸½å’Œã€‚\"\n",
        "            \"å›å‚³ä»¥ Markdown æ¢åˆ—ï¼Œæ ¼å¼ï¼š\\n\"\n",
        "            \"### Morning\\n- [ä»»å‹™ID] ä»»å‹™åç¨±ï¼ˆé ä¼° xx åˆ†ï¼‰â€” å‚™è¨»\\n...\"\n",
        "            \"### Afternoon\\n...\\n### Evening\\n...\\n\"\n",
        "        )\n",
        "        items = []\n",
        "        for _, r in cand.iterrows():\n",
        "            items.append({\n",
        "                \"id\": r[\"id\"], \"task\": r[\"task\"], \"est_min\": int(r[\"est_min\"]),\n",
        "                \"priority\": r[\"priority\"]\n",
        "            })\n",
        "        user_content = json.dumps({\"today\": today, \"tasks\": items}, ensure_ascii=False)\n",
        "        try:\n",
        "            model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
        "            resp = model.generate_content(sys_prompt + \"\\n\\n\" + user_content)\n",
        "            plan_md = resp.text\n",
        "        except Exception as e:\n",
        "            plan_md = f\"âš ï¸ Gemini å¤±æ•—ï¼š{e}\\n\\næ”¹ç”¨è¦å‰‡å¼è¦åŠƒã€‚\"\n",
        "    else:\n",
        "        plan_md = \"ğŸ”§ æœªè¨­å®š GEMINI_API_KEYï¼Œä½¿ç”¨è¦å‰‡å¼è¦åŠƒã€‚\\n\\n\"\n",
        "\n",
        "    # è¦å‰‡å¼ï¼šæŠŠé«˜å„ªå…ˆä»»å‹™å¹³å‡åˆ‡åˆ°ä¸Šåˆ/ä¸‹åˆ/æ™šä¸Š\n",
        "    buckets = {\"morning\": [], \"afternoon\": [], \"evening\": []}\n",
        "    total = len(cand)\n",
        "    for i, (_, r) in enumerate(cand.iterrows()):\n",
        "        if i % 3 == 0:\n",
        "            buckets[\"morning\"].append(r)\n",
        "        elif i % 3 == 1:\n",
        "            buckets[\"afternoon\"].append(r)\n",
        "        else:\n",
        "            buckets[\"evening\"].append(r)\n",
        "\n",
        "    def sec_md(name, rows):\n",
        "        if not rows: return f\"### {name.title()}\\nï¼ˆç„¡ï¼‰\\n\"\n",
        "        lines = [f\"### {name.title()}\"]\n",
        "        for r in rows:\n",
        "            lines.append(f\"- [{r['id']}] {r['task']}ï¼ˆé ä¼° {int(r['est_min'])} åˆ†ï¼ŒP:{r['priority']}ï¼‰\")\n",
        "        return \"\\n\".join(lines) + \"\\n\"\n",
        "\n",
        "    rule_md = sec_md(\"morning\", buckets[\"morning\"]) + \"\\n\" + \\\n",
        "              sec_md(\"afternoon\", buckets[\"afternoon\"]) + \"\\n\" + \\\n",
        "              sec_md(\"evening\", buckets[\"evening\"])\n",
        "\n",
        "    return (plan_md + \"\\n---\\n\" + rule_md).strip()\n",
        "\n",
        "# ä»Šæ—¥å®Œæˆç‡\n",
        "def today_summary():\n",
        "    global tasks_df\n",
        "    today = tznow().date().isoformat()\n",
        "    planned = tasks_df[\n",
        "        ((tasks_df[\"due_date\"]==today) | (tasks_df[\"planned_for\"].str.lower()==\"today\"))\n",
        "    ]\n",
        "    done = planned[planned[\"status\"]==\"done\"]\n",
        "    total = len(planned)\n",
        "    done_n = len(done)\n",
        "    rate = (done_n/total*100) if total>0 else 0\n",
        "    return f\"ğŸ“… ä»Šæ—¥è¨ˆç•«ä»»å‹™ï¼š{total}ï¼›âœ… å®Œæˆï¼š{done_n}ï¼›ğŸ“ˆ å®Œæˆç‡ï¼š{rate:.1f}%\"\n",
        "\n",
        "# =========================\n",
        "# çˆ¬èŸ²ï¼šæ“·å–æ–‡å­—æˆ–é€£çµä¸¦å¯åŠ å…¥ä»»å‹™\n",
        "# =========================\n",
        "def crawl(url, selector, mode, limit):\n",
        "    try:\n",
        "        resp = requests.get(url, timeout=15, headers={\"User-Agent\":\"Mozilla/5.0\"})\n",
        "        resp.raise_for_status()\n",
        "    except Exception as e:\n",
        "        return pd.DataFrame(columns=CLIPS_HEADER), f\"âš ï¸ è«‹æ±‚å¤±æ•—ï¼š{e}\"\n",
        "\n",
        "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
        "    nodes = soup.select(selector)\n",
        "    rows = []\n",
        "    for i, n in enumerate(nodes[:int(limit) if limit else 20]):\n",
        "        text = n.get_text(strip=True) if mode in (\"text\",\"both\") else \"\"\n",
        "        href = n.get(\"href\") if mode in (\"href\",\"both\") else \"\"\n",
        "        # ç›¸å°é€£çµè™•ç†\n",
        "        if href and href.startswith(\"/\"):\n",
        "            from urllib.parse import urljoin\n",
        "            href = urljoin(url, href)\n",
        "        rows.append({\n",
        "            \"clip_id\": str(uuid.uuid4())[:8],\n",
        "            \"url\": url,\n",
        "            \"selector\": selector,\n",
        "            \"text\": text,\n",
        "            \"href\": href,\n",
        "            \"created_at\": tznow().isoformat(),\n",
        "            \"added_to_task\": \"\"\n",
        "        })\n",
        "    df = pd.DataFrame(rows, columns=CLIPS_HEADER)\n",
        "    return df, f\"âœ… æ“·å– {len(df)} ç­†\"\n",
        "\n",
        "def add_clips_as_tasks(clip_ids, default_priority, est_min):\n",
        "    global clips_df, tasks_df\n",
        "    if not clip_ids:\n",
        "        return \"âš ï¸ è«‹å…ˆå‹¾é¸è¦åŠ å…¥çš„çˆ¬èŸ²é …ç›®\", clips_df, tasks_df\n",
        "    sel = clips_df[clips_df[\"clip_id\"].isin(clip_ids)]\n",
        "    _now = tznow().isoformat()\n",
        "    new_tasks = []\n",
        "    for _, r in sel.iterrows():\n",
        "        title = r[\"text\"] or r[\"href\"] or \"ï¼ˆæœªå‘½åï¼‰\"\n",
        "        note = f\"ä¾†æºï¼š{r['url']}\\né¸æ“‡å™¨ï¼š{r['selector']}\\né€£çµï¼š{r['href']}\"\n",
        "        new_tasks.append({\n",
        "            \"id\": str(uuid.uuid4())[:8],\n",
        "            \"task\": title[:120],\n",
        "            \"status\": \"todo\",\n",
        "            \"priority\": default_priority or \"M\",\n",
        "            \"est_min\": int(est_min) if est_min else 25,\n",
        "            \"start_time\": \"\",\n",
        "            \"end_time\": \"\",\n",
        "            \"actual_min\": 0,\n",
        "            \"pomodoros\": 0,\n",
        "            \"due_date\": \"\",\n",
        "            \"labels\": \"from:crawler\",\n",
        "            \"notes\": note,\n",
        "            \"created_at\": _now,\n",
        "            \"updated_at\": _now,\n",
        "            \"completed_at\": \"\",\n",
        "            \"planned_for\": \"\"\n",
        "        })\n",
        "    if new_tasks:\n",
        "        tasks_df = pd.concat([tasks_df, pd.DataFrame(new_tasks)], ignore_index=True)\n",
        "        # æ¨™è¨˜å·²åŠ å…¥\n",
        "        clips_df.loc[clips_df[\"clip_id\"].isin(clip_ids), \"added_to_task\"] = \"yes\"\n",
        "        write_df(ws_tasks, tasks_df, TASKS_HEADER)\n",
        "        write_df(ws_clips, clips_df, CLIPS_HEADER)\n",
        "        return f\"âœ… å·²åŠ å…¥ {len(new_tasks)} é …ç‚ºä»»å‹™\", clips_df, tasks_df\n",
        "    return \"âš ï¸ ç„¡å¯åŠ å…¥é …ç›®\", clips_df, tasks_df\n",
        "\n",
        "\n",
        "def read_ptt_posts_df():\n",
        "    return read_df(ws_ptt_posts, PTT_HEADER).copy()\n",
        "\n",
        "def read_terms_df():\n",
        "    return read_df(ws_ptt_terms, TERMS_HEADER).copy()\n",
        "\n",
        "ptt_posts_df = read_ptt_posts_df()\n",
        "terms_df = read_terms_df()\n"
      ],
      "metadata": {
        "id": "D0GH11YEQIkB"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Gradio ä»‹é¢\n",
        "# =========================\n",
        "def _refresh():\n",
        "    global tasks_df, logs_df, clips_df\n",
        "    tasks_df, logs_df, clips_df = refresh_all()\n",
        "    return tasks_df, logs_df, clips_df, list_task_choices(), today_summary()\n",
        "\n",
        "with gr.Blocks(title=\"å¾…è¾¦æ¸…å–®ï¼‹ç•ªèŒ„é˜ï¼‹AI è¨ˆç•«ï¼ˆSheet/Gradio/çˆ¬èŸ²ï¼‰\") as demo:\n",
        "    gr.Markdown(\"# âœ… å¾…è¾¦æ¸…å–®èˆ‡ç•ªèŒ„é˜ï¼ˆGoogle Sheetï¼‹Gradioï¼‹Crawlerï¼‹AI è¨ˆç•«ï¼‰\")\n",
        "    with gr.Row():\n",
        "        btn_refresh = gr.Button(\"ğŸ”„ é‡æ–°æ•´ç†ï¼ˆSheet â†’ Appï¼‰\")\n",
        "        out_summary = gr.Markdown(today_summary())\n",
        "\n",
        "    with gr.Tab(\"Tasks\"):\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=2):\n",
        "                task = gr.Textbox(label=\"ä»»å‹™åç¨±\", placeholder=\"å¯« HW3 å ±å‘Š / ä¿®æ­£ SQL / â€¦\")\n",
        "                priority = gr.Dropdown([\"H\",\"M\",\"L\"], value=\"M\", label=\"å„ªå…ˆç´š\")\n",
        "                est_min = gr.Number(value=25, label=\"é ä¼°æ™‚é–“ï¼ˆåˆ†é˜ï¼‰\", precision=0)\n",
        "                due_date = gr.Textbox(label=\"åˆ°æœŸæ—¥ï¼ˆYYYY-MM-DDï¼Œå¯ç©ºç™½ï¼‰\")\n",
        "                labels = gr.Textbox(label=\"æ¨™ç±¤ï¼ˆé€—è™Ÿåˆ†éš”ï¼Œå¯ç©ºç™½ï¼‰\")\n",
        "                notes = gr.Textbox(label=\"å‚™è¨»ï¼ˆå¯ç©ºç™½ï¼‰\")\n",
        "                planned_for = gr.Dropdown([\"\",\"today\",\"tomorrow\"], value=\"\", label=\"è¦åŠƒæ­¸å±¬\")\n",
        "                btn_add = gr.Button(\"â• æ–°å¢ä»»å‹™\")\n",
        "                msg_add = gr.Markdown()\n",
        "            with gr.Column(scale=3):\n",
        "                grid_tasks = gr.Dataframe(value=tasks_df, label=\"ä»»å‹™æ¸…å–®ï¼ˆç›´æ¥å¾ Sheet ä¾†ï¼‰\", interactive=False)\n",
        "\n",
        "        with gr.Row():\n",
        "            task_choice = gr.Dropdown(choices=list_task_choices(), label=\"é¸å–ä»»å‹™ï¼ˆç”¨æ–¼æ›´æ–°ï¼‰\")\n",
        "            new_status = gr.Dropdown([\"todo\",\"in-progress\",\"done\"], value=\"in-progress\", label=\"æ›´æ–°ç‹€æ…‹\")\n",
        "            btn_update = gr.Button(\"âœï¸ æ›´æ–°ç‹€æ…‹\")\n",
        "            btn_done = gr.Button(\"âœ… ç›´æ¥æ¨™è¨˜å®Œæˆ\")\n",
        "            msg_update = gr.Markdown()\n",
        "\n",
        "    with gr.Tab(\"Pomodoro\"):\n",
        "        with gr.Row():\n",
        "            sel_task = gr.Dropdown(choices=list_task_choices(), label=\"é¸æ“‡ä»»å‹™\")\n",
        "            cycles = gr.Number(value=1, precision=0, label=\"ç•ªèŒ„æ•¸ï¼ˆåƒ…ä½œç´€éŒ„ï¼‰\")\n",
        "        with gr.Row():\n",
        "            btn_start_work = gr.Button(\"â–¶ï¸ é–‹å§‹å·¥ä½œ\")\n",
        "            note_work = gr.Textbox(label=\"å·¥ä½œå‚™è¨»ï¼ˆå¯ç©ºç™½ï¼‰\")\n",
        "            btn_end_work = gr.Button(\"â¹ï¸ çµæŸå·¥ä½œä¸¦è¨˜éŒ„\")\n",
        "        with gr.Row():\n",
        "            btn_start_break = gr.Button(\"ğŸµ é–‹å§‹ä¼‘æ¯\")\n",
        "            note_break = gr.Textbox(label=\"ä¼‘æ¯å‚™è¨»ï¼ˆå¯ç©ºç™½ï¼‰\")\n",
        "            btn_end_break = gr.Button(\"â¹ï¸ çµæŸä¼‘æ¯ä¸¦è¨˜éŒ„\")\n",
        "        msg_pomo = gr.Markdown()\n",
        "        grid_logs = gr.Dataframe(value=logs_df, label=\"ç•ªèŒ„é˜ç´€éŒ„\", interactive=False)\n",
        "\n",
        "    with gr.Tab(\"AI Plan\"):\n",
        "        gr.Markdown(\"æŠŠ**ä»Šå¤©çš„ä»»å‹™**æ’æˆ **morning / afternoon / evening** ä¸‰æ®µè¡Œå‹•è¨ˆç•«ã€‚è‹¥æœªè¨­ GEMINI_API_KEYï¼Œæœƒç”¨è¦å‰‡å¼ã€‚\")\n",
        "        btn_plan = gr.Button(\"ğŸ§  ç”¢ç”Ÿä»Šæ—¥è¨ˆç•«\")\n",
        "        out_plan = gr.Markdown()\n",
        "\n",
        "    with gr.Tab(\"Crawler\"):\n",
        "        url = gr.Textbox(label=\"ç›®æ¨™ URL\", placeholder=\"https://example.com\")\n",
        "        selector = gr.Textbox(label=\"CSS Selector\", placeholder=\"a.news-item / h2.title / div.card a\")\n",
        "        mode = gr.Radio([\"text\",\"href\",\"both\"], value=\"text\", label=\"æ“·å–å…§å®¹\")\n",
        "        limit = gr.Number(value=20, precision=0, label=\"æœ€å¤šæ“·å–å¹¾ç­†\")\n",
        "        btn_crawl = gr.Button(\"ğŸ•·ï¸ é–‹å§‹æ“·å–\")\n",
        "        msg_crawl = gr.Markdown()\n",
        "        grid_clips = gr.Dataframe(value=clips_df, label=\"æ“·å–çµæœï¼ˆæœƒåŒæ­¥å¯«å…¥ Sheetï¼‰\", interactive=True)\n",
        "        clip_ids = gr.Textbox(label=\"è¦åŠ å…¥ä»»å‹™çš„ clip_idï¼ˆå¤šå€‹ä»¥é€—è™Ÿåˆ†éš”ï¼‰\")\n",
        "        default_priority = gr.Dropdown([\"H\",\"M\",\"L\"], value=\"L\", label=\"æ–°å¢ä»»å‹™å„ªå…ˆç´š\")\n",
        "        clip_est = gr.Number(value=25, precision=0, label=\"æ–°å¢ä»»å‹™é ä¼°åˆ†é˜\")\n",
        "        btn_add_clips = gr.Button(\"â• å°‡å‹¾é¸çš„æ“·å–é …ç›®åŠ å…¥ç‚ºä»»å‹™\")\n",
        "        msg_add_clips = gr.Markdown()\n",
        "\n",
        "    with gr.Tab(\"Summary\"):\n",
        "        btn_summary = gr.Button(\"ğŸ“Š é‡æ–°è¨ˆç®—ä»Šæ—¥å®Œæˆç‡\")\n",
        "        out_summary2 = gr.Markdown()\n",
        "\n",
        "    # === ç¶å®šå‹•ä½œ ===\n",
        "    btn_refresh.click(_refresh, outputs=[grid_tasks, grid_logs, grid_clips, task_choice, out_summary])\n",
        "\n",
        "    btn_add.click(\n",
        "        add_task,\n",
        "        inputs=[task, priority, est_min, due_date, labels, notes, planned_for],\n",
        "        outputs=[msg_add, grid_tasks]\n",
        "    )\n",
        "\n",
        "    btn_update.click(\n",
        "        update_task_status,\n",
        "        inputs=[task_choice, new_status],\n",
        "        outputs=[msg_update, grid_tasks]\n",
        "    )\n",
        "\n",
        "    btn_done.click(\n",
        "        mark_done,\n",
        "        inputs=[task_choice],\n",
        "        outputs=[msg_update, grid_tasks]\n",
        "    )\n",
        "\n",
        "    btn_start_work.click(\n",
        "        start_phase, inputs=[sel_task, gr.State(\"work\"), cycles], outputs=[msg_pomo]\n",
        "    )\n",
        "    btn_end_work.click(\n",
        "        end_phase, inputs=[sel_task, note_work], outputs=[msg_pomo]\n",
        "    )\n",
        "    btn_start_break.click(\n",
        "        start_phase, inputs=[sel_task, gr.State(\"break\"), cycles], outputs=[msg_pomo]\n",
        "    )\n",
        "    btn_end_break.click(\n",
        "        end_phase, inputs=[sel_task, note_break], outputs=[msg_pomo]\n",
        "    )\n",
        "\n",
        "    btn_plan.click(generate_today_plan, outputs=[out_plan])\n",
        "\n",
        "    def _crawl_and_save(u, s, m, l):\n",
        "        df, msg = crawl(u, s, m, l)\n",
        "        # å¯«å…¥ web_clipsï¼ˆè¦†è“‹å¼è¿½åŠ ï¼šåˆä½µèˆŠè³‡æ–™ï¼‰\n",
        "        global clips_df\n",
        "        if not df.empty:\n",
        "            clips_df = pd.concat([clips_df, df], ignore_index=True)\n",
        "            write_df(ws_clips, clips_df, CLIPS_HEADER)\n",
        "        return msg, clips_df\n",
        "\n",
        "    btn_crawl.click(_crawl_and_save, inputs=[url, selector, mode, limit], outputs=[msg_crawl, grid_clips])\n",
        "\n",
        "    def _add_clips(clip_ids_str, pr, est):\n",
        "        ids = [c.strip() for c in (clip_ids_str or \"\").split(\",\") if c.strip()]\n",
        "        msg, new_clips, new_tasks = add_clips_as_tasks(ids, pr, est)\n",
        "        return msg, new_clips, new_tasks\n",
        "\n",
        "    btn_add_clips.click(\n",
        "        _add_clips,\n",
        "        inputs=[clip_ids, default_priority, clip_est],\n",
        "        outputs=[msg_add_clips, grid_clips, grid_tasks]\n",
        "    )\n",
        "\n",
        "    btn_summary.click(today_summary, outputs=[out_summary2])\n",
        "\n",
        "demo.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 647
        },
        "id": "6paI6RAnan_Z",
        "outputId": "24f6fe8f-51a2-4252-a3ba-4ee0f9164047"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://43244acc5db3b6274f.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://43244acc5db3b6274f.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    }
  ]
}